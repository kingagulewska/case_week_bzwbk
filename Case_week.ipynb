{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py:3035: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6002\n",
      "Number of testing examples: 15204\n",
      "train_x_orig shape: (6002, 244)\n",
      "in training set number of zeros is: 3601 and number of ones: 2401 \n",
      "train_y shape: (1, 6002)\n",
      "test_x_orig shape: (15204, 244)\n",
      "test_y shape: (1, 15204)\n",
      "train_x's shape: (244, 6002)\n",
      "test_x's shape: (244, 15204)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, svm, model_selection, metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/171475/train.csv')\n",
    "OutOfSample = pd.read_csv('C:/Users/171475/test.csv')\n",
    "\n",
    "df['var3'] = df['var3'].replace(-999999, 2)\n",
    "\n",
    "\n",
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.2, random_state=seed)\n",
    "\n",
    "train_1 = train_df[train_df[\"TARGET\"] == 1]\n",
    "train_0 = train_df[train_df[\"TARGET\"] == 0]\n",
    "number_of_0_samples = int(np.floor(train_1.shape[0]*6/4))\n",
    "train_0_balanced = train_0.sample(n=number_of_0_samples, random_state = seed)\n",
    "\n",
    "train_balanced = shuffle(pd.concat([train_1, train_0_balanced]))\n",
    "\n",
    "number_of_ones = sum(train_balanced['TARGET'])\n",
    "number_of_zeros = train_balanced.shape[0] - sum(train_balanced['TARGET'])\n",
    "\n",
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)\n",
    "\n",
    "OutOfSample.fillna(0, inplace=True)\n",
    "\n",
    "train_y = train_balanced[[\"TARGET\"]].T\n",
    "test_y = test_df[[\"TARGET\"]].T\n",
    "\n",
    "m_train = train_balanced.shape[0]\n",
    "m_test = test_df.shape[0]\n",
    "\n",
    "some_columns = (train_balanced.max() != 0) & (train_balanced.min() >= 0)\n",
    "train_x_orig = train_balanced.loc[:, some_columns]\n",
    "test_x_orig = test_df.loc[:, some_columns]\n",
    "\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"in training set number of zeros is: {} and number of ones: {} \".format(number_of_zeros, number_of_ones))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n",
    "\n",
    "max_of_train = train_x_orig.max()\n",
    "train_x = train_x_orig / max_of_train\n",
    "train_x = train_x.T\n",
    "\n",
    "test_x = test_x_orig / max_of_train\n",
    "test_x = test_x.T\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert (A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.1\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.1\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.04967142 -0.01382643  0.06476885]\n",
      " [ 0.15230299 -0.02341534 -0.0234137 ]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.15792128  0.07674347]]\n",
      "b2 = [[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3, 2, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "\n",
    "    cost = -1 / m * (np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), 1 - Y.T))\n",
    "\n",
    "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    parameters['W1'] = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n",
    "    parameters['b1'] = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n",
    "    parameters['W2'] = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n",
    "    parameters['b2'] = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    p = np.zeros((1, m))\n",
    "\n",
    "    # Forward propagation\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b2']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "    probas, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "    # probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "\n",
    "    # print results\n",
    "    # print (\"predictions: \" + str(p))\n",
    "    # print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n",
    "\n",
    "    return p, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.001, num_iterations = 4500, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        \n",
    "        # Compute cost\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'relu')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'sigmoid')\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            print(\"Gini after iteration {}: {}\".format(i, (2*metrics.roc_auc_score(Y.T, A2.T))-1))\n",
    "            predictions_test, probability_test = predict(np.array(test_x), np.array(test_y), parameters)\n",
    "            print(\"Gini after iteration in test set {}: {}\".format(i, (2*metrics.roc_auc_score(test_y.T, probability_test.T))-1))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6957581420684308\n",
      "Gini after iteration 0: 0.3288103945396261\n",
      "Accuracy: 0.161141804788\n",
      "Gini after iteration in test set 0: 0.2985042739142423\n",
      "Cost after iteration 100: 0.6838865707251125\n",
      "Gini after iteration 100: 0.5365818255167909\n",
      "Accuracy: 0.422717705867\n",
      "Gini after iteration in test set 100: 0.5197083555906581\n",
      "Cost after iteration 200: 0.6782064457953941\n",
      "Gini after iteration 200: 0.6694379285868692\n",
      "Accuracy: 0.471323335964\n",
      "Gini after iteration in test set 200: 0.6442790991220579\n",
      "Cost after iteration 300: 0.6735370380284267\n",
      "Gini after iteration 300: 0.7638163585685449\n",
      "Accuracy: 0.476913970008\n",
      "Gini after iteration in test set 300: 0.7347129281941553\n",
      "Cost after iteration 400: 0.6690643459361667\n",
      "Gini after iteration 400: 0.8292230130438341\n",
      "Accuracy: 0.477045514338\n",
      "Gini after iteration in test set 400: 0.8008426050398072\n",
      "Cost after iteration 500: 0.6648809656484448\n",
      "Gini after iteration 500: 0.8759553694245468\n",
      "Accuracy: 0.475401210208\n",
      "Gini after iteration in test set 500: 0.849816582338069\n",
      "Cost after iteration 600: 0.6607683462393148\n",
      "Gini after iteration 600: 0.9093615649593376\n",
      "Accuracy: 0.473691133912\n",
      "Gini after iteration in test set 600: 0.8860748507484839\n",
      "Cost after iteration 700: 0.6566391828891965\n",
      "Gini after iteration 700: 0.9337934381455659\n",
      "Accuracy: 0.473954222573\n",
      "Gini after iteration in test set 700: 0.9140585295504853\n",
      "Cost after iteration 800: 0.652543630743349\n",
      "Gini after iteration 800: 0.9518764802363542\n",
      "Accuracy: 0.473691133912\n",
      "Gini after iteration in test set 800: 0.9346781892738449\n",
      "Cost after iteration 900: 0.6483996181008623\n",
      "Gini after iteration 900: 0.9655544800422764\n",
      "Accuracy: 0.477177058669\n",
      "Gini after iteration in test set 900: 0.9505614827537283\n",
      "Cost after iteration 1000: 0.6443050358882698\n",
      "Gini after iteration 1000: 0.9753536924180324\n",
      "Accuracy: 0.477374375164\n",
      "Gini after iteration in test set 1000: 0.9628758544075822\n",
      "Cost after iteration 1100: 0.6402247124661351\n",
      "Gini after iteration 1100: 0.9818656046882253\n",
      "Accuracy: 0.478755590634\n",
      "Gini after iteration in test set 1100: 0.9712524712543336\n",
      "Cost after iteration 1200: 0.6361754042234621\n",
      "Gini after iteration 1200: 0.986103633344479\n",
      "Accuracy: 0.476979742173\n",
      "Gini after iteration in test set 1200: 0.9765599191637289\n",
      "Cost after iteration 1300: 0.6320333338675428\n",
      "Gini after iteration 1300: 0.9891610005596805\n",
      "Accuracy: 0.478689818469\n",
      "Gini after iteration in test set 1300: 0.9804574951026361\n",
      "Cost after iteration 1400: 0.6278659614035181\n",
      "Gini after iteration 1400: 0.9913916271811674\n",
      "Accuracy: 0.481649565904\n",
      "Gini after iteration in test set 1400: 0.9831032058560925\n",
      "Cost after iteration 1500: 0.6235951984073578\n",
      "Gini after iteration 1500: 0.9929227396573284\n",
      "Accuracy: 0.483951591686\n",
      "Gini after iteration in test set 1500: 0.985373989081054\n",
      "Cost after iteration 1600: 0.6192911880478876\n",
      "Gini after iteration 1600: 0.993948647472976\n",
      "Accuracy: 0.488424098921\n",
      "Gini after iteration in test set 1600: 0.9871154495761412\n",
      "Cost after iteration 1700: 0.614993326343106\n",
      "Gini after iteration 1700: 0.9947265793746729\n",
      "Accuracy: 0.491318074191\n",
      "Gini after iteration in test set 1700: 0.9887490140094457\n",
      "Cost after iteration 1800: 0.6106744222695607\n",
      "Gini after iteration 1800: 0.995433958427717\n",
      "Accuracy: 0.495395948435\n",
      "Gini after iteration in test set 1800: 0.9900218715249087\n",
      "Cost after iteration 1900: 0.606223521356286\n",
      "Gini after iteration 1900: 0.9960626884035753\n",
      "Accuracy: 0.498421468035\n",
      "Gini after iteration in test set 1900: 0.9914179743327007\n",
      "Cost after iteration 2000: 0.6015641141085355\n",
      "Gini after iteration 2000: 0.9966393711960015\n",
      "Accuracy: 0.503880557748\n",
      "Gini after iteration in test set 2000: 0.9925046095657986\n",
      "Cost after iteration 2100: 0.5969768984980326\n",
      "Gini after iteration 2100: 0.9973284759046406\n",
      "Accuracy: 0.507234938174\n",
      "Gini after iteration in test set 2100: 0.993506598306912\n",
      "Cost after iteration 2200: 0.5922917504150214\n",
      "Gini after iteration 2200: 0.997822808486837\n",
      "Accuracy: 0.510983951592\n",
      "Gini after iteration in test set 2200: 0.9943115300146868\n",
      "Cost after iteration 2300: 0.5876158851650649\n",
      "Gini after iteration 2300: 0.9982260006678232\n",
      "Accuracy: 0.514469876348\n",
      "Gini after iteration in test set 2300: 0.995138808396345\n",
      "Cost after iteration 2400: 0.5828135398583675\n",
      "Gini after iteration 2400: 0.9985424475430895\n",
      "Accuracy: 0.518481978427\n",
      "Gini after iteration in test set 2400: 0.9959498346515425\n",
      "Cost after iteration 2500: 0.5779061824314622\n",
      "Gini after iteration 2500: 0.9987795513787243\n",
      "Accuracy: 0.523151802157\n",
      "Gini after iteration in test set 2500: 0.9965391999597311\n",
      "Cost after iteration 2600: 0.5729889050087748\n",
      "Gini after iteration 2600: 0.9990006940781062\n",
      "Accuracy: 0.527821625888\n",
      "Gini after iteration in test set 2600: 0.9970342126448541\n",
      "Cost after iteration 2700: 0.5680384674786092\n",
      "Gini after iteration 2700: 0.9992035624330833\n",
      "Accuracy: 0.532951854775\n",
      "Gini after iteration in test set 2700: 0.9973895021872092\n",
      "Cost after iteration 2800: 0.5631830009557423\n",
      "Gini after iteration 2800: 0.9993562341711504\n",
      "Accuracy: 0.536174690871\n",
      "Gini after iteration in test set 2800: 0.9976466017988621\n",
      "Cost after iteration 2900: 0.5582086587861705\n",
      "Gini after iteration 2900: 0.9994520009886652\n",
      "Accuracy: 0.541041831097\n",
      "Gini after iteration in test set 2900: 0.9978091230634716\n",
      "Cost after iteration 3000: 0.5532907609441168\n",
      "Gini after iteration 3000: 0.9995503123351477\n",
      "Accuracy: 0.542751907393\n",
      "Gini after iteration in test set 3000: 0.997919953536976\n",
      "Cost after iteration 3100: 0.5483922445342011\n",
      "Gini after iteration 3100: 0.999667591988481\n",
      "Accuracy: 0.543606945541\n",
      "Gini after iteration in test set 3100: 0.9981287482172039\n",
      "Cost after iteration 3200: 0.5434390104287574\n",
      "Gini after iteration 3200: 0.9997291233253385\n",
      "Accuracy: 0.545317021836\n",
      "Gini after iteration in test set 3200: 0.9982980412011722\n",
      "Cost after iteration 3300: 0.5384576845223236\n",
      "Gini after iteration 3300: 0.9997772380549113\n",
      "Accuracy: 0.54801368061\n",
      "Gini after iteration in test set 3300: 0.9984971297503189\n",
      "Cost after iteration 3400: 0.5334888872966544\n",
      "Gini after iteration 3400: 0.999804533911111\n",
      "Accuracy: 0.549723756906\n",
      "Gini after iteration in test set 3400: 0.9987140504937768\n",
      "Cost after iteration 3500: 0.5285221171779526\n",
      "Gini after iteration 3500: 0.9998309044840501\n",
      "Accuracy: 0.551170744541\n",
      "Gini after iteration in test set 3500: 0.9989027557399068\n",
      "Cost after iteration 3600: 0.5235522779458626\n",
      "Gini after iteration 3600: 0.9998612075108482\n",
      "Accuracy: 0.552486187845\n",
      "Gini after iteration in test set 3600: 0.9989476748116533\n",
      "Cost after iteration 3700: 0.5184890440610501\n",
      "Gini after iteration 3700: 0.9999190377146614\n",
      "Accuracy: 0.554985530124\n",
      "Gini after iteration in test set 3700: 0.9989804047885535\n",
      "Cost after iteration 3800: 0.5135843365428422\n",
      "Gini after iteration 3800: 0.9999324543219459\n",
      "Accuracy: 0.554985530124\n",
      "Gini after iteration in test set 3800: 0.9990205836567487\n",
      "Cost after iteration 3900: 0.5086200752925922\n",
      "Gini after iteration 3900: 0.9999400879088496\n",
      "Accuracy: 0.554853985793\n",
      "Gini after iteration in test set 3900: 0.9990494763260127\n",
      "Cost after iteration 4000: 0.5037069786630244\n",
      "Gini after iteration 4000: 0.9999472588541223\n",
      "Accuracy: 0.554525124967\n",
      "Gini after iteration in test set 4000: 0.9990806262350629\n",
      "Cost after iteration 4100: 0.49883486735703925\n",
      "Gini after iteration 4100: 0.9999546611202104\n",
      "Accuracy: 0.553604314654\n",
      "Gini after iteration in test set 4100: 0.9990941696737803\n",
      "Cost after iteration 4200: 0.49405090184658723\n",
      "Gini after iteration 4200: 0.9999636826320053\n",
      "Accuracy: 0.549723756906\n",
      "Gini after iteration in test set 4200: 0.9991068102165832\n",
      "Cost after iteration 4300: 0.4891154273659416\n",
      "Gini after iteration 4300: 0.9999703909356474\n",
      "Accuracy: 0.550381478558\n",
      "Gini after iteration in test set 4300: 0.9991162906236857\n"
     ]
    }
   ],
   "source": [
    "n_x = train_x.shape[0]\n",
    "n_h = 50\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "parameters = two_layer_model(np.array(train_x), np.array(train_y), layers_dims=(n_x, n_h, n_y),num_iterations = 6000, print_cost=True)\n",
    "\n",
    "predictions_train, probability_train = predict(np.array(train_x), np.array(train_y), parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_train, probability_train = predict(np.array(train_x), np.array(train_y), parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
